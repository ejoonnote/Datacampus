{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb008167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3255b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f47c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c61ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b2e4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = (X_test/255).reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44686a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a37f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "# 생성자 만들기\n",
    "# GAN에서 관행적으로 units=256 2의 몇제곱값 형태로 함\n",
    "generator = Sequential()\n",
    "generator.add(Dense(units=256 , input_dim=NOISE_DIM , activation=LeakyReLU(0.2) ))\n",
    "generator.add(Dense(units=512, activation=LeakyReLU(0.2)))\n",
    "generator.add(Dense(units=1024, activation=LeakyReLU(0.2)))\n",
    "generator.add(Dense(units=784, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "079c536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdbae77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#판별자 만들기\n",
    "# units을 감소시키는 방향으로 함\n",
    "detector = Sequential()\n",
    "detector.add(Dense(units=1024, input_dim = 784, activation= LeakyReLU(0.2)))\n",
    "detector.add(Dense(units=512, activation=LeakyReLU(0.2)))\n",
    "detector.add(Dense(units=256, activation=LeakyReLU(0.2)))\n",
    "detector.add(Dense(units=1, activation='sigmoid'))\n",
    "detector.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49cd042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5055dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 식별자,판별자를 감싼 전체 신경망의 존재가 필요함\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "total_input = Input(shape=(NOISE_DIM,))\n",
    "x = generator(inputs = total_input)\n",
    "# generator 신경망 만들어짐\n",
    "# detector가 받게 함\n",
    "total_output = detector(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c8949aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(total_input, total_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db07cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b9e0536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0  BATCH:  0  g_loss: [0.0008104928419925272, 1.0]  d_loss:  [3.845320224761963, 0.0]\n",
      "EPOCH:  0  BATCH:  1  g_loss: [0.0008001255919225514, 1.0]  d_loss:  [3.8511099815368652, 0.0]\n",
      "EPOCH:  0  BATCH:  2  g_loss: [0.0007907907711341977, 1.0]  d_loss:  [3.856325149536133, 0.0]\n",
      "EPOCH:  0  BATCH:  3  g_loss: [0.00078175263479352, 1.0]  d_loss:  [3.8624610900878906, 0.0]\n",
      "EPOCH:  0  BATCH:  4  g_loss: [0.0007763259345665574, 1.0]  d_loss:  [3.868183135986328, 0.0]\n",
      "EPOCH:  0  BATCH:  5  g_loss: [0.0007720327703282237, 1.0]  d_loss:  [3.871040105819702, 0.0]\n",
      "EPOCH:  0  BATCH:  6  g_loss: [0.000769128673709929, 1.0]  d_loss:  [3.869246482849121, 0.0]\n",
      "EPOCH:  0  BATCH:  7  g_loss: [0.0007668107282370329, 1.0]  d_loss:  [3.8739185333251953, 0.0]\n",
      "EPOCH:  0  BATCH:  8  g_loss: [0.0007648523896932602, 1.0]  d_loss:  [3.8752779960632324, 0.0]\n",
      "EPOCH:  0  BATCH:  9  g_loss: [0.0007628387538716197, 1.0]  d_loss:  [3.8736605644226074, 0.0]\n",
      "EPOCH:  0  BATCH:  10  g_loss: [0.0007607515435665846, 1.0]  d_loss:  [3.8770551681518555, 0.0]\n",
      "EPOCH:  0  BATCH:  11  g_loss: [0.0007588288863189518, 1.0]  d_loss:  [3.8809854984283447, 0.0]\n",
      "EPOCH:  0  BATCH:  12  g_loss: [0.0007571472087875009, 1.0]  d_loss:  [3.880398750305176, 0.0]\n",
      "EPOCH:  0  BATCH:  13  g_loss: [0.0007559077348560095, 1.0]  d_loss:  [3.8811168670654297, 0.0]\n",
      "EPOCH:  0  BATCH:  14  g_loss: [0.0007547596469521523, 1.0]  d_loss:  [3.8809609413146973, 0.0]\n",
      "EPOCH:  0  BATCH:  15  g_loss: [0.0007537272176705301, 1.0]  d_loss:  [3.8845462799072266, 0.0]\n",
      "EPOCH:  0  BATCH:  16  g_loss: [0.0007528115529567003, 1.0]  d_loss:  [3.8807594776153564, 0.0]\n",
      "EPOCH:  0  BATCH:  17  g_loss: [0.0007518577040173113, 1.0]  d_loss:  [3.8816609382629395, 0.0]\n",
      "EPOCH:  0  BATCH:  18  g_loss: [0.0007507065311074257, 1.0]  d_loss:  [3.8858351707458496, 0.0]\n",
      "EPOCH:  0  BATCH:  19  g_loss: [0.0007497483165934682, 1.0]  d_loss:  [3.88407039642334, 0.0]\n",
      "EPOCH:  0  BATCH:  20  g_loss: [0.0007485131500288844, 1.0]  d_loss:  [3.884761095046997, 0.0]\n",
      "EPOCH:  0  BATCH:  21  g_loss: [0.0007467492250725627, 1.0]  d_loss:  [3.886558771133423, 0.0]\n",
      "EPOCH:  0  BATCH:  22  g_loss: [0.000744755263440311, 1.0]  d_loss:  [3.8906784057617188, 0.0]\n",
      "EPOCH:  0  BATCH:  23  g_loss: [0.0007422256749123335, 1.0]  d_loss:  [3.890359878540039, 0.0]\n",
      "EPOCH:  0  BATCH:  24  g_loss: [0.0007393617415800691, 1.0]  d_loss:  [3.891510248184204, 0.0]\n",
      "EPOCH:  0  BATCH:  25  g_loss: [0.0007370016537606716, 1.0]  d_loss:  [3.8950467109680176, 0.0]\n",
      "EPOCH:  0  BATCH:  26  g_loss: [0.0007350962259806693, 1.0]  d_loss:  [3.892709970474243, 0.0]\n",
      "EPOCH:  0  BATCH:  27  g_loss: [0.0007335745031014085, 1.0]  d_loss:  [3.895745277404785, 0.0]\n",
      "EPOCH:  0  BATCH:  28  g_loss: [0.0007320251897908747, 1.0]  d_loss:  [3.8974900245666504, 0.0]\n",
      "EPOCH:  0  BATCH:  29  g_loss: [0.0007303443271666765, 1.0]  d_loss:  [3.8968119621276855, 0.0]\n",
      "EPOCH:  0  BATCH:  30  g_loss: [0.0007287468761205673, 1.0]  d_loss:  [3.8963024616241455, 0.0]\n",
      "EPOCH:  0  BATCH:  31  g_loss: [0.0007271651411429048, 1.0]  d_loss:  [3.899628162384033, 0.0]\n",
      "EPOCH:  0  BATCH:  32  g_loss: [0.0007258558180183172, 1.0]  d_loss:  [3.901948928833008, 0.0]\n",
      "EPOCH:  0  BATCH:  33  g_loss: [0.0007243439322337508, 1.0]  d_loss:  [3.903003692626953, 0.0]\n",
      "EPOCH:  0  BATCH:  34  g_loss: [0.0007223084685392678, 1.0]  d_loss:  [3.901923656463623, 0.0]\n",
      "EPOCH:  0  BATCH:  35  g_loss: [0.0007200237596407533, 1.0]  d_loss:  [3.9012255668640137, 0.0]\n",
      "EPOCH:  0  BATCH:  36  g_loss: [0.0007177382940426469, 1.0]  d_loss:  [3.905381202697754, 0.0]\n",
      "EPOCH:  0  BATCH:  37  g_loss: [0.00071632832987234, 1.0]  d_loss:  [3.90708589553833, 0.0]\n",
      "EPOCH:  0  BATCH:  38  g_loss: [0.0007152085890993476, 1.0]  d_loss:  [3.909590244293213, 0.0]\n",
      "EPOCH:  0  BATCH:  39  g_loss: [0.0007144815754145384, 1.0]  d_loss:  [3.9078516960144043, 0.0]\n",
      "EPOCH:  0  BATCH:  40  g_loss: [0.0007139138760976493, 1.0]  d_loss:  [3.9063477516174316, 0.0]\n",
      "EPOCH:  0  BATCH:  41  g_loss: [0.0007134169572964311, 1.0]  d_loss:  [3.906982183456421, 0.0]\n",
      "EPOCH:  0  BATCH:  42  g_loss: [0.0007132551399990916, 1.0]  d_loss:  [3.9101171493530273, 0.0]\n",
      "EPOCH:  0  BATCH:  43  g_loss: [0.0007130164885893464, 1.0]  d_loss:  [3.90908145904541, 0.0]\n",
      "EPOCH:  0  BATCH:  44  g_loss: [0.0007129046716727316, 1.0]  d_loss:  [3.9103634357452393, 0.0]\n",
      "EPOCH:  0  BATCH:  45  g_loss: [0.0007127398275770247, 1.0]  d_loss:  [3.9103236198425293, 0.0]\n",
      "EPOCH:  0  BATCH:  46  g_loss: [0.0007127805147320032, 1.0]  d_loss:  [3.9084784984588623, 0.0]\n",
      "EPOCH:  0  BATCH:  47  g_loss: [0.0007127280696295202, 1.0]  d_loss:  [3.9100632667541504, 0.0]\n",
      "EPOCH:  0  BATCH:  48  g_loss: [0.0007126227719709277, 1.0]  d_loss:  [3.910905122756958, 0.0]\n",
      "EPOCH:  0  BATCH:  49  g_loss: [0.0007126567070372403, 1.0]  d_loss:  [3.910067558288574, 0.0]\n",
      "EPOCH:  0  BATCH:  50  g_loss: [0.0007125799893401563, 1.0]  d_loss:  [3.906489372253418, 0.0]\n",
      "EPOCH:  0  BATCH:  51  g_loss: [0.0007125698029994965, 1.0]  d_loss:  [3.90745210647583, 0.0]\n",
      "EPOCH:  0  BATCH:  52  g_loss: [0.0007124625844880939, 1.0]  d_loss:  [3.9089467525482178, 0.0]\n",
      "EPOCH:  0  BATCH:  53  g_loss: [0.0007123885443434119, 1.0]  d_loss:  [3.908771276473999, 0.0]\n",
      "EPOCH:  0  BATCH:  54  g_loss: [0.0007123532705008984, 1.0]  d_loss:  [3.9101576805114746, 0.0]\n",
      "EPOCH:  0  BATCH:  55  g_loss: [0.0007122542592696846, 1.0]  d_loss:  [3.910529851913452, 0.0]\n",
      "EPOCH:  0  BATCH:  56  g_loss: [0.0007121728849597275, 1.0]  d_loss:  [3.912357807159424, 0.0]\n",
      "EPOCH:  0  BATCH:  57  g_loss: [0.0007121319649741054, 1.0]  d_loss:  [3.9115943908691406, 0.0]\n",
      "EPOCH:  0  BATCH:  58  g_loss: [0.0007119960500858724, 1.0]  d_loss:  [3.908785343170166, 0.0]\n",
      "EPOCH:  0  BATCH:  59  g_loss: [0.0007120067020878196, 1.0]  d_loss:  [3.9079389572143555, 0.0]\n",
      "EPOCH:  0  BATCH:  60  g_loss: [0.0007118995999917388, 1.0]  d_loss:  [3.910829544067383, 0.0]\n",
      "EPOCH:  0  BATCH:  61  g_loss: [0.0007118336507119238, 1.0]  d_loss:  [3.9117181301116943, 0.0]\n",
      "EPOCH:  0  BATCH:  62  g_loss: [0.000711681554093957, 1.0]  d_loss:  [3.908496141433716, 0.0]\n",
      "EPOCH:  0  BATCH:  63  g_loss: [0.0007115640910342336, 1.0]  d_loss:  [3.9091620445251465, 0.0]\n",
      "EPOCH:  0  BATCH:  64  g_loss: [0.000711324333678931, 1.0]  d_loss:  [3.9104256629943848, 0.0]\n",
      "EPOCH:  0  BATCH:  65  g_loss: [0.0007109905127435923, 1.0]  d_loss:  [3.9093801975250244, 0.0]\n",
      "EPOCH:  0  BATCH:  66  g_loss: [0.0007104995311237872, 1.0]  d_loss:  [3.908493757247925, 0.0]\n",
      "EPOCH:  0  BATCH:  67  g_loss: [0.000709853193257004, 1.0]  d_loss:  [3.9124507904052734, 0.0]\n",
      "EPOCH:  0  BATCH:  68  g_loss: [0.0007093605236150324, 1.0]  d_loss:  [3.9118945598602295, 0.0]\n",
      "EPOCH:  0  BATCH:  69  g_loss: [0.0007092857849784195, 1.0]  d_loss:  [3.910472869873047, 0.0]\n",
      "EPOCH:  0  BATCH:  70  g_loss: [0.0007093853782862425, 1.0]  d_loss:  [3.909730911254883, 0.0]\n",
      "EPOCH:  0  BATCH:  71  g_loss: [0.0007094750762917101, 1.0]  d_loss:  [3.9170613288879395, 0.0]\n",
      "EPOCH:  0  BATCH:  72  g_loss: [0.0007094948086887598, 1.0]  d_loss:  [3.9135360717773438, 0.0]\n",
      "EPOCH:  0  BATCH:  73  g_loss: [0.0007095671608112752, 1.0]  d_loss:  [3.910861015319824, 0.0]\n",
      "EPOCH:  0  BATCH:  74  g_loss: [0.0007095416076481342, 1.0]  d_loss:  [3.914665699005127, 0.0]\n",
      "EPOCH:  0  BATCH:  75  g_loss: [0.0007095466135069728, 1.0]  d_loss:  [3.9085094928741455, 0.0]\n",
      "EPOCH:  0  BATCH:  76  g_loss: [0.0007095283945091069, 1.0]  d_loss:  [3.911184072494507, 0.0]\n",
      "EPOCH:  0  BATCH:  77  g_loss: [0.000709484564140439, 1.0]  d_loss:  [3.910029411315918, 0.0]\n",
      "EPOCH:  0  BATCH:  78  g_loss: [0.0007094506872817874, 1.0]  d_loss:  [3.9117588996887207, 0.0]\n",
      "EPOCH:  0  BATCH:  79  g_loss: [0.0007094000466167927, 1.0]  d_loss:  [3.912705898284912, 0.0]\n",
      "EPOCH:  0  BATCH:  80  g_loss: [0.0007093296153470874, 1.0]  d_loss:  [3.911287546157837, 0.0]\n",
      "EPOCH:  0  BATCH:  81  g_loss: [0.000709259242285043, 1.0]  d_loss:  [3.9119162559509277, 0.0]\n",
      "EPOCH:  0  BATCH:  82  g_loss: [0.0007092134328559041, 1.0]  d_loss:  [3.910223960876465, 0.0]\n",
      "EPOCH:  0  BATCH:  83  g_loss: [0.0007091233273968101, 1.0]  d_loss:  [3.916292667388916, 0.0]\n",
      "EPOCH:  0  BATCH:  84  g_loss: [0.0007089965511113405, 1.0]  d_loss:  [3.9117507934570312, 0.0]\n",
      "EPOCH:  0  BATCH:  85  g_loss: [0.0007089284481480718, 1.0]  d_loss:  [3.9099247455596924, 0.0]\n",
      "EPOCH:  0  BATCH:  86  g_loss: [0.0007088375277817249, 1.0]  d_loss:  [3.9119739532470703, 0.0]\n",
      "EPOCH:  0  BATCH:  87  g_loss: [0.000708686769939959, 1.0]  d_loss:  [3.9120774269104004, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0  BATCH:  88  g_loss: [0.0007086091209203005, 1.0]  d_loss:  [3.9121618270874023, 0.0]\n",
      "EPOCH:  0  BATCH:  89  g_loss: [0.0007085047545842826, 1.0]  d_loss:  [3.9056613445281982, 0.0]\n",
      "EPOCH:  0  BATCH:  90  g_loss: [0.0007083822274580598, 1.0]  d_loss:  [3.9129252433776855, 0.0]\n",
      "EPOCH:  0  BATCH:  91  g_loss: [0.0007082941010594368, 1.0]  d_loss:  [3.9148387908935547, 0.0]\n",
      "EPOCH:  0  BATCH:  92  g_loss: [0.000708110979758203, 1.0]  d_loss:  [3.9114608764648438, 0.0]\n",
      "EPOCH:  0  BATCH:  93  g_loss: [0.0007079864153638482, 1.0]  d_loss:  [3.9125924110412598, 0.0]\n",
      "EPOCH:  0  BATCH:  94  g_loss: [0.0007078201742842793, 1.0]  d_loss:  [3.9125359058380127, 0.0]\n",
      "EPOCH:  0  BATCH:  95  g_loss: [0.0007076655747368932, 1.0]  d_loss:  [3.9122872352600098, 0.0]\n",
      "EPOCH:  0  BATCH:  96  g_loss: [0.0007075101020745933, 1.0]  d_loss:  [3.9111781120300293, 0.0]\n",
      "EPOCH:  0  BATCH:  97  g_loss: [0.000707370403688401, 1.0]  d_loss:  [3.9120125770568848, 0.0]\n",
      "EPOCH:  0  BATCH:  98  g_loss: [0.0007072523003444076, 1.0]  d_loss:  [3.9126110076904297, 0.0]\n",
      "EPOCH:  0  BATCH:  99  g_loss: [0.0007071657455526292, 1.0]  d_loss:  [3.9152941703796387, 0.0]\n",
      "EPOCH:  0  BATCH:  100  g_loss: [0.000707096653059125, 1.0]  d_loss:  [3.9138877391815186, 0.0]\n",
      "EPOCH:  0  BATCH:  101  g_loss: [0.0007069932762533426, 1.0]  d_loss:  [3.912700653076172, 0.0]\n",
      "EPOCH:  0  BATCH:  102  g_loss: [0.0007068979321047664, 1.0]  d_loss:  [3.917179584503174, 0.0]\n",
      "EPOCH:  0  BATCH:  103  g_loss: [0.000706739432644099, 1.0]  d_loss:  [3.910339832305908, 0.0]\n",
      "EPOCH:  0  BATCH:  104  g_loss: [0.0007064852397888899, 1.0]  d_loss:  [3.9160962104797363, 0.0]\n",
      "EPOCH:  0  BATCH:  105  g_loss: [0.000706061371602118, 1.0]  d_loss:  [3.918083429336548, 0.0]\n",
      "EPOCH:  0  BATCH:  106  g_loss: [0.0007055720197968185, 1.0]  d_loss:  [3.913398027420044, 0.0]\n",
      "EPOCH:  0  BATCH:  107  g_loss: [0.0007049839477986097, 1.0]  d_loss:  [3.9166688919067383, 0.0]\n",
      "EPOCH:  0  BATCH:  108  g_loss: [0.0007042932556942105, 1.0]  d_loss:  [3.9143762588500977, 0.0]\n",
      "EPOCH:  0  BATCH:  109  g_loss: [0.0007034360896795988, 1.0]  d_loss:  [3.9180164337158203, 0.0]\n",
      "EPOCH:  0  BATCH:  110  g_loss: [0.0007027939427644014, 1.0]  d_loss:  [3.916710615158081, 0.0]\n",
      "EPOCH:  0  BATCH:  111  g_loss: [0.0007022982463240623, 1.0]  d_loss:  [3.9149727821350098, 0.0]\n",
      "EPOCH:  0  BATCH:  112  g_loss: [0.0007020378834567964, 1.0]  d_loss:  [3.912796974182129, 0.0]\n",
      "EPOCH:  0  BATCH:  113  g_loss: [0.0007018622709438205, 1.0]  d_loss:  [3.9148919582366943, 0.0]\n",
      "EPOCH:  0  BATCH:  114  g_loss: [0.0007016623858362436, 1.0]  d_loss:  [3.918238401412964, 0.0]\n",
      "EPOCH:  0  BATCH:  115  g_loss: [0.0007014776347205043, 1.0]  d_loss:  [3.9184365272521973, 0.0]\n",
      "EPOCH:  0  BATCH:  116  g_loss: [0.0007012917776592076, 1.0]  d_loss:  [3.916138172149658, 0.0]\n",
      "EPOCH:  0  BATCH:  117  g_loss: [0.0007011202396824956, 1.0]  d_loss:  [3.920811414718628, 0.0]\n",
      "EPOCH:  0  BATCH:  118  g_loss: [0.0007010367698967457, 1.0]  d_loss:  [3.9204025268554688, 0.0]\n",
      "EPOCH:  0  BATCH:  119  g_loss: [0.0007009684341028333, 1.0]  d_loss:  [3.9185497760772705, 0.0]\n",
      "EPOCH:  0  BATCH:  120  g_loss: [0.0007009521941654384, 1.0]  d_loss:  [3.9163124561309814, 0.0]\n",
      "EPOCH:  0  BATCH:  121  g_loss: [0.0007009341497905552, 1.0]  d_loss:  [3.9177842140197754, 0.0]\n",
      "EPOCH:  0  BATCH:  122  g_loss: [0.0007008796674199402, 1.0]  d_loss:  [3.9204251766204834, 0.0]\n",
      "EPOCH:  0  BATCH:  123  g_loss: [0.0007008309476077557, 1.0]  d_loss:  [3.9184653759002686, 0.0]\n",
      "EPOCH:  0  BATCH:  124  g_loss: [0.0007007407839410007, 1.0]  d_loss:  [3.9198684692382812, 0.0]\n",
      "EPOCH:  0  BATCH:  125  g_loss: [0.0007007030653767288, 1.0]  d_loss:  [3.9197659492492676, 0.0]\n",
      "EPOCH:  0  BATCH:  126  g_loss: [0.0007005985244177282, 1.0]  d_loss:  [3.9186458587646484, 0.0]\n",
      "EPOCH:  0  BATCH:  127  g_loss: [0.000700492353644222, 1.0]  d_loss:  [3.917250871658325, 0.0]\n",
      "EPOCH:  0  BATCH:  128  g_loss: [0.0007003443315625191, 1.0]  d_loss:  [3.9197518825531006, 0.0]\n",
      "EPOCH:  0  BATCH:  129  g_loss: [0.0007002869388088584, 1.0]  d_loss:  [3.914222240447998, 0.0]\n",
      "EPOCH:  0  BATCH:  130  g_loss: [0.0007002109778113663, 1.0]  d_loss:  [3.9192562103271484, 0.0]\n",
      "EPOCH:  0  BATCH:  131  g_loss: [0.0007001431658864021, 1.0]  d_loss:  [3.9179747104644775, 0.0]\n",
      "EPOCH:  0  BATCH:  132  g_loss: [0.0007001174381002784, 1.0]  d_loss:  [3.9190053939819336, 0.0]\n",
      "EPOCH:  0  BATCH:  133  g_loss: [0.0007000163896009326, 1.0]  d_loss:  [3.9163155555725098, 0.0]\n",
      "EPOCH:  0  BATCH:  134  g_loss: [0.0006999284378252923, 1.0]  d_loss:  [3.9192473888397217, 0.0]\n",
      "EPOCH:  0  BATCH:  135  g_loss: [0.0006997650489211082, 1.0]  d_loss:  [3.9222240447998047, 0.0]\n",
      "EPOCH:  0  BATCH:  136  g_loss: [0.0006996426382102072, 1.0]  d_loss:  [3.9196853637695312, 0.0]\n",
      "EPOCH:  0  BATCH:  137  g_loss: [0.0006994971772655845, 1.0]  d_loss:  [3.9213409423828125, 0.0]\n",
      "EPOCH:  0  BATCH:  138  g_loss: [0.000699463183991611, 1.0]  d_loss:  [3.9214935302734375, 0.0]\n",
      "EPOCH:  0  BATCH:  139  g_loss: [0.0006993974093347788, 1.0]  d_loss:  [3.9219584465026855, 0.0]\n",
      "EPOCH:  0  BATCH:  140  g_loss: [0.0006993892602622509, 1.0]  d_loss:  [3.91530179977417, 0.0]\n",
      "EPOCH:  0  BATCH:  141  g_loss: [0.000699469936080277, 1.0]  d_loss:  [3.9196059703826904, 0.0]\n",
      "EPOCH:  0  BATCH:  142  g_loss: [0.0006994549185037613, 1.0]  d_loss:  [3.9190149307250977, 0.0]\n",
      "EPOCH:  0  BATCH:  143  g_loss: [0.0006994706927798688, 1.0]  d_loss:  [3.9176673889160156, 0.0]\n",
      "EPOCH:  0  BATCH:  144  g_loss: [0.0006993634160608053, 1.0]  d_loss:  [3.9185376167297363, 0.0]\n",
      "EPOCH:  0  BATCH:  145  g_loss: [0.0006993191200308502, 1.0]  d_loss:  [3.920644760131836, 0.0]\n",
      "EPOCH:  0  BATCH:  146  g_loss: [0.0006992383860051632, 1.0]  d_loss:  [3.919797658920288, 0.0]\n",
      "EPOCH:  0  BATCH:  147  g_loss: [0.0006991870468482375, 1.0]  d_loss:  [3.920754909515381, 0.0]\n",
      "EPOCH:  0  BATCH:  148  g_loss: [0.0006991845439188182, 1.0]  d_loss:  [3.9244394302368164, 0.0]\n",
      "EPOCH:  0  BATCH:  149  g_loss: [0.0006991666741669178, 1.0]  d_loss:  [3.9182229042053223, 0.0]\n",
      "EPOCH:  0  BATCH:  150  g_loss: [0.0006991691188886762, 1.0]  d_loss:  [3.9193313121795654, 0.0]\n",
      "EPOCH:  0  BATCH:  151  g_loss: [0.0006991352420300245, 1.0]  d_loss:  [3.9237585067749023, 0.0]\n",
      "EPOCH:  0  BATCH:  152  g_loss: [0.0006991177215240896, 1.0]  d_loss:  [3.919090747833252, 0.0]\n",
      "EPOCH:  0  BATCH:  153  g_loss: [0.0006990880938246846, 1.0]  d_loss:  [3.9193031787872314, 0.0]\n",
      "EPOCH:  0  BATCH:  154  g_loss: [0.0006990401889197528, 1.0]  d_loss:  [3.91805362701416, 0.0]\n",
      "EPOCH:  0  BATCH:  155  g_loss: [0.0006989827379584312, 1.0]  d_loss:  [3.91664981842041, 0.0]\n",
      "EPOCH:  0  BATCH:  156  g_loss: [0.0006989104440435767, 1.0]  d_loss:  [3.9200892448425293, 0.0]\n",
      "EPOCH:  0  BATCH:  157  g_loss: [0.0006988759851083159, 1.0]  d_loss:  [3.9175546169281006, 0.0]\n",
      "EPOCH:  0  BATCH:  158  g_loss: [0.0006987400120124221, 1.0]  d_loss:  [3.919161319732666, 0.0]\n",
      "EPOCH:  0  BATCH:  159  g_loss: [0.0006986607331782579, 1.0]  d_loss:  [3.9193098545074463, 0.0]\n",
      "EPOCH:  0  BATCH:  160  g_loss: [0.0006985665531829, 1.0]  d_loss:  [3.9202487468719482, 0.0]\n",
      "EPOCH:  0  BATCH:  161  g_loss: [0.0006983564817346632, 1.0]  d_loss:  [3.9174582958221436, 0.0]\n",
      "EPOCH:  0  BATCH:  162  g_loss: [0.0006981612532399595, 1.0]  d_loss:  [3.920156478881836, 0.0]\n",
      "EPOCH:  0  BATCH:  163  g_loss: [0.0006978948367759585, 1.0]  d_loss:  [3.9182353019714355, 0.0]\n",
      "EPOCH:  0  BATCH:  164  g_loss: [0.000697718933224678, 1.0]  d_loss:  [3.9204864501953125, 0.0]\n",
      "EPOCH:  0  BATCH:  165  g_loss: [0.000697619398124516, 1.0]  d_loss:  [3.919969081878662, 0.0]\n",
      "EPOCH:  0  BATCH:  166  g_loss: [0.0006975079304538667, 1.0]  d_loss:  [3.9187326431274414, 0.0]\n",
      "EPOCH:  0  BATCH:  167  g_loss: [0.0006972278351895511, 1.0]  d_loss:  [3.917102336883545, 0.0]\n",
      "EPOCH:  0  BATCH:  168  g_loss: [0.0006968483794480562, 1.0]  d_loss:  [3.9229514598846436, 0.0]\n",
      "EPOCH:  0  BATCH:  169  g_loss: [0.0006965631619095802, 1.0]  d_loss:  [3.9191765785217285, 0.0]\n",
      "EPOCH:  0  BATCH:  170  g_loss: [0.00069613684900105, 1.0]  d_loss:  [3.920105457305908, 0.0]\n",
      "EPOCH:  0  BATCH:  171  g_loss: [0.0006956376018933952, 1.0]  d_loss:  [3.9240567684173584, 0.0]\n",
      "EPOCH:  0  BATCH:  172  g_loss: [0.000694962393026799, 1.0]  d_loss:  [3.9254350662231445, 0.0]\n",
      "EPOCH:  0  BATCH:  173  g_loss: [0.0006942687323316932, 1.0]  d_loss:  [3.9242868423461914, 0.0]\n",
      "EPOCH:  0  BATCH:  174  g_loss: [0.0006936330464668572, 1.0]  d_loss:  [3.922694683074951, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0  BATCH:  175  g_loss: [0.0006932458491064608, 1.0]  d_loss:  [3.9244844913482666, 0.0]\n",
      "EPOCH:  0  BATCH:  176  g_loss: [0.0006929542287252843, 1.0]  d_loss:  [3.9226694107055664, 0.0]\n",
      "EPOCH:  0  BATCH:  177  g_loss: [0.0006927899667061865, 1.0]  d_loss:  [3.9255752563476562, 0.0]\n",
      "EPOCH:  0  BATCH:  178  g_loss: [0.0006927895010448992, 1.0]  d_loss:  [3.9220762252807617, 0.0]\n",
      "EPOCH:  0  BATCH:  179  g_loss: [0.0006928318180143833, 1.0]  d_loss:  [3.923901319503784, 0.0]\n",
      "EPOCH:  0  BATCH:  180  g_loss: [0.0006927156355232, 1.0]  d_loss:  [3.923595905303955, 0.0]\n",
      "EPOCH:  0  BATCH:  181  g_loss: [0.000692694797180593, 1.0]  d_loss:  [3.925264835357666, 0.0]\n",
      "EPOCH:  0  BATCH:  182  g_loss: [0.0006925659254193306, 1.0]  d_loss:  [3.925393581390381, 0.0]\n",
      "EPOCH:  0  BATCH:  183  g_loss: [0.0006925659254193306, 1.0]  d_loss:  [3.92264461517334, 0.0]\n",
      "EPOCH:  0  BATCH:  184  g_loss: [0.0006925020134076476, 1.0]  d_loss:  [3.922525644302368, 0.0]\n",
      "EPOCH:  0  BATCH:  185  g_loss: [0.000692434492520988, 1.0]  d_loss:  [3.924015998840332, 0.0]\n",
      "EPOCH:  0  BATCH:  186  g_loss: [0.0006923885084688663, 1.0]  d_loss:  [3.92341947555542, 0.0]\n",
      "EPOCH:  0  BATCH:  187  g_loss: [0.0006923972396180034, 1.0]  d_loss:  [3.9255728721618652, 0.0]\n",
      "EPOCH:  0  BATCH:  188  g_loss: [0.0006924602203071117, 1.0]  d_loss:  [3.9226765632629395, 0.0]\n",
      "EPOCH:  0  BATCH:  189  g_loss: [0.0006924709305167198, 1.0]  d_loss:  [3.924495220184326, 0.0]\n",
      "EPOCH:  0  BATCH:  190  g_loss: [0.0006924483459442854, 1.0]  d_loss:  [3.9269330501556396, 0.0]\n",
      "EPOCH:  0  BATCH:  191  g_loss: [0.000692429137416184, 1.0]  d_loss:  [3.9277544021606445, 0.0]\n",
      "EPOCH:  0  BATCH:  192  g_loss: [0.0006923897890374064, 1.0]  d_loss:  [3.922865390777588, 0.0]\n",
      "EPOCH:  0  BATCH:  193  g_loss: [0.0006923525361344218, 1.0]  d_loss:  [3.926495313644409, 0.0]\n",
      "EPOCH:  0  BATCH:  194  g_loss: [0.0006922541651874781, 1.0]  d_loss:  [3.925081729888916, 0.0]\n",
      "EPOCH:  0  BATCH:  195  g_loss: [0.0006922052125446498, 1.0]  d_loss:  [3.923314094543457, 0.0]\n",
      "EPOCH:  0  BATCH:  196  g_loss: [0.0006921819294802845, 1.0]  d_loss:  [3.924124002456665, 0.0]\n",
      "EPOCH:  0  BATCH:  197  g_loss: [0.0006921715103089809, 1.0]  d_loss:  [3.922595500946045, 0.0]\n",
      "EPOCH:  0  BATCH:  198  g_loss: [0.0006921321037225425, 1.0]  d_loss:  [3.923511505126953, 0.0]\n",
      "EPOCH:  0  BATCH:  199  g_loss: [0.00069219502620399, 1.0]  d_loss:  [3.9254941940307617, 0.0]\n",
      "EPOCH:  0  BATCH:  200  g_loss: [0.0006921879248693585, 1.0]  d_loss:  [3.923226833343506, 0.0]\n",
      "EPOCH:  0  BATCH:  201  g_loss: [0.0006921227322891355, 1.0]  d_loss:  [3.9224555492401123, 0.0]\n",
      "EPOCH:  0  BATCH:  202  g_loss: [0.0006920892046764493, 1.0]  d_loss:  [3.9239087104797363, 0.0]\n",
      "EPOCH:  0  BATCH:  203  g_loss: [0.0006920893210917711, 1.0]  d_loss:  [3.9258899688720703, 0.0]\n",
      "EPOCH:  0  BATCH:  204  g_loss: [0.0006920536980032921, 1.0]  d_loss:  [3.9256720542907715, 0.0]\n",
      "EPOCH:  0  BATCH:  205  g_loss: [0.0006920313462615013, 1.0]  d_loss:  [3.922990083694458, 0.0]\n",
      "EPOCH:  0  BATCH:  206  g_loss: [0.0006919990992173553, 1.0]  d_loss:  [3.925253391265869, 0.0]\n",
      "EPOCH:  0  BATCH:  207  g_loss: [0.000691988505423069, 1.0]  d_loss:  [3.9246301651000977, 0.0]\n",
      "EPOCH:  0  BATCH:  208  g_loss: [0.0006920129526406527, 1.0]  d_loss:  [3.924652099609375, 0.0]\n",
      "EPOCH:  0  BATCH:  209  g_loss: [0.0006919867591932416, 1.0]  d_loss:  [3.9225010871887207, 0.0]\n",
      "EPOCH:  0  BATCH:  210  g_loss: [0.0006919791921973228, 1.0]  d_loss:  [3.921934127807617, 0.0]\n",
      "EPOCH:  0  BATCH:  211  g_loss: [0.0006919174920767546, 1.0]  d_loss:  [3.9212207794189453, 0.0]\n",
      "EPOCH:  0  BATCH:  212  g_loss: [0.000691879540681839, 1.0]  d_loss:  [3.924814462661743, 0.0]\n",
      "EPOCH:  0  BATCH:  213  g_loss: [0.000691823719535023, 1.0]  d_loss:  [3.924184799194336, 0.0]\n",
      "EPOCH:  0  BATCH:  214  g_loss: [0.0006917818682268262, 1.0]  d_loss:  [3.922844171524048, 0.0]\n",
      "EPOCH:  0  BATCH:  215  g_loss: [0.0006917616119608283, 1.0]  d_loss:  [3.9265403747558594, 0.0]\n",
      "EPOCH:  0  BATCH:  216  g_loss: [0.0006916834390722215, 1.0]  d_loss:  [3.926701784133911, 0.0]\n",
      "EPOCH:  0  BATCH:  217  g_loss: [0.0006916911224834621, 1.0]  d_loss:  [3.9266676902770996, 0.0]\n",
      "EPOCH:  0  BATCH:  218  g_loss: [0.0006916320417076349, 1.0]  d_loss:  [3.9240500926971436, 0.0]\n",
      "EPOCH:  0  BATCH:  219  g_loss: [0.0006915609119459987, 1.0]  d_loss:  [3.924898624420166, 0.0]\n",
      "EPOCH:  0  BATCH:  220  g_loss: [0.0006915158010087907, 1.0]  d_loss:  [3.919680595397949, 0.0]\n",
      "EPOCH:  0  BATCH:  221  g_loss: [0.0006914359400980175, 1.0]  d_loss:  [3.922548770904541, 0.0]\n",
      "EPOCH:  0  BATCH:  222  g_loss: [0.0006913680117577314, 1.0]  d_loss:  [3.9216229915618896, 0.0]\n",
      "EPOCH:  0  BATCH:  223  g_loss: [0.0006912936805747449, 1.0]  d_loss:  [3.9245824813842773, 0.0]\n",
      "EPOCH:  0  BATCH:  224  g_loss: [0.0006912560784257948, 1.0]  d_loss:  [3.9220824241638184, 0.0]\n",
      "EPOCH:  0  BATCH:  225  g_loss: [0.0006912370445206761, 1.0]  d_loss:  [3.9285778999328613, 0.0]\n",
      "EPOCH:  0  BATCH:  226  g_loss: [0.0006912118988111615, 1.0]  d_loss:  [3.924727201461792, 0.0]\n",
      "EPOCH:  0  BATCH:  227  g_loss: [0.0006911681266501546, 1.0]  d_loss:  [3.9257802963256836, 0.0]\n",
      "EPOCH:  0  BATCH:  228  g_loss: [0.0006911666132509708, 1.0]  d_loss:  [3.926298141479492, 0.0]\n",
      "EPOCH:  0  BATCH:  229  g_loss: [0.0006912328535690904, 1.0]  d_loss:  [3.9265737533569336, 0.0]\n",
      "EPOCH:  0  BATCH:  230  g_loss: [0.0006911851232871413, 1.0]  d_loss:  [3.9258508682250977, 0.0]\n",
      "EPOCH:  0  BATCH:  231  g_loss: [0.0006911601522006094, 1.0]  d_loss:  [3.922738790512085, 0.0]\n",
      "EPOCH:  0  BATCH:  232  g_loss: [0.0006911626551300287, 1.0]  d_loss:  [3.9253289699554443, 0.0]\n",
      "EPOCH:  0  BATCH:  233  g_loss: [0.0006911534001119435, 1.0]  d_loss:  [3.9242663383483887, 0.0]\n",
      "EPOCH:  0  BATCH:  234  g_loss: [0.0006911592208780348, 1.0]  d_loss:  [3.9291858673095703, 0.0]\n",
      "EPOCH:  0  BATCH:  235  g_loss: [0.000691198802087456, 1.0]  d_loss:  [3.924189805984497, 0.0]\n",
      "EPOCH:  0  BATCH:  236  g_loss: [0.000691169174388051, 1.0]  d_loss:  [3.92706561088562, 0.0]\n",
      "EPOCH:  0  BATCH:  237  g_loss: [0.0006911603268235922, 1.0]  d_loss:  [3.9271187782287598, 0.0]\n",
      "EPOCH:  0  BATCH:  238  g_loss: [0.000691140303388238, 1.0]  d_loss:  [3.9233551025390625, 0.0]\n",
      "EPOCH:  0  BATCH:  239  g_loss: [0.0006911556702107191, 1.0]  d_loss:  [3.927347183227539, 0.0]\n",
      "EPOCH:  0  BATCH:  240  g_loss: [0.0006911114905960858, 1.0]  d_loss:  [3.922463893890381, 0.0]\n",
      "EPOCH:  0  BATCH:  241  g_loss: [0.0006910837255418301, 1.0]  d_loss:  [3.926438808441162, 0.0]\n",
      "EPOCH:  0  BATCH:  242  g_loss: [0.0006911486270837486, 1.0]  d_loss:  [3.9262351989746094, 0.0]\n",
      "EPOCH:  0  BATCH:  243  g_loss: [0.0006911130039952695, 1.0]  d_loss:  [3.9248228073120117, 0.0]\n",
      "EPOCH:  0  BATCH:  244  g_loss: [0.0006911370437592268, 1.0]  d_loss:  [3.9287784099578857, 0.0]\n",
      "EPOCH:  0  BATCH:  245  g_loss: [0.000691102584823966, 1.0]  d_loss:  [3.924074649810791, 0.0]\n",
      "EPOCH:  0  BATCH:  246  g_loss: [0.0006911029922775924, 1.0]  d_loss:  [3.9277920722961426, 0.0]\n",
      "EPOCH:  0  BATCH:  247  g_loss: [0.000691115390509367, 1.0]  d_loss:  [3.926703929901123, 0.0]\n",
      "EPOCH:  0  BATCH:  248  g_loss: [0.0006911205127835274, 1.0]  d_loss:  [3.9263386726379395, 0.0]\n",
      "EPOCH:  0  BATCH:  249  g_loss: [0.000691073713824153, 1.0]  d_loss:  [3.9250733852386475, 0.0]\n",
      "EPOCH:  0  BATCH:  250  g_loss: [0.000691091816406697, 1.0]  d_loss:  [3.927022695541382, 0.0]\n",
      "EPOCH:  0  BATCH:  251  g_loss: [0.0006911038653925061, 1.0]  d_loss:  [3.9255905151367188, 0.0]\n",
      "EPOCH:  0  BATCH:  252  g_loss: [0.0006910728989169002, 1.0]  d_loss:  [3.9241080284118652, 0.0]\n",
      "EPOCH:  0  BATCH:  253  g_loss: [0.0006910934462212026, 1.0]  d_loss:  [3.921374797821045, 0.0]\n",
      "EPOCH:  0  BATCH:  254  g_loss: [0.0006910727825015783, 1.0]  d_loss:  [3.923754930496216, 0.0]\n",
      "EPOCH:  0  BATCH:  255  g_loss: [0.0006910833762958646, 1.0]  d_loss:  [3.9259815216064453, 0.0]\n",
      "EPOCH:  0  BATCH:  256  g_loss: [0.0006910897209309042, 1.0]  d_loss:  [3.9229090213775635, 0.0]\n",
      "EPOCH:  0  BATCH:  257  g_loss: [0.0006910375086590648, 1.0]  d_loss:  [3.925297737121582, 0.0]\n",
      "EPOCH:  0  BATCH:  258  g_loss: [0.0006910684751346707, 1.0]  d_loss:  [3.9245619773864746, 0.0]\n",
      "EPOCH:  0  BATCH:  259  g_loss: [0.0006910555530339479, 1.0]  d_loss:  [3.9273033142089844, 0.0]\n",
      "EPOCH:  0  BATCH:  260  g_loss: [0.000691026565618813, 1.0]  d_loss:  [3.9278039932250977, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0  BATCH:  261  g_loss: [0.0006910304073244333, 1.0]  d_loss:  [3.9214282035827637, 0.0]\n",
      "EPOCH:  0  BATCH:  262  g_loss: [0.0006910605006851256, 1.0]  d_loss:  [3.923649787902832, 0.0]\n",
      "EPOCH:  0  BATCH:  263  g_loss: [0.0006910448428243399, 1.0]  d_loss:  [3.926511287689209, 0.0]\n",
      "EPOCH:  0  BATCH:  264  g_loss: [0.0006909917574375868, 1.0]  d_loss:  [3.923692464828491, 0.0]\n",
      "EPOCH:  0  BATCH:  265  g_loss: [0.0006910555530339479, 1.0]  d_loss:  [3.924952507019043, 0.0]\n",
      "EPOCH:  0  BATCH:  266  g_loss: [0.0006910160882398486, 1.0]  d_loss:  [3.924450397491455, 0.0]\n",
      "EPOCH:  0  BATCH:  267  g_loss: [0.0006910532247275114, 1.0]  d_loss:  [3.9267585277557373, 0.0]\n",
      "EPOCH:  0  BATCH:  268  g_loss: [0.0006909932708367705, 1.0]  d_loss:  [3.9266676902770996, 0.0]\n",
      "EPOCH:  0  BATCH:  269  g_loss: [0.0006910042138770223, 1.0]  d_loss:  [3.923208236694336, 0.0]\n",
      "EPOCH:  0  BATCH:  270  g_loss: [0.0006909959483891726, 1.0]  d_loss:  [3.923133611679077, 0.0]\n",
      "EPOCH:  0  BATCH:  271  g_loss: [0.0006909837247803807, 1.0]  d_loss:  [3.923405885696411, 0.0]\n",
      "EPOCH:  0  BATCH:  272  g_loss: [0.0006909609655849636, 1.0]  d_loss:  [3.9267001152038574, 0.0]\n",
      "EPOCH:  0  BATCH:  273  g_loss: [0.000690969405695796, 1.0]  d_loss:  [3.9235706329345703, 0.0]\n",
      "EPOCH:  0  BATCH:  274  g_loss: [0.0006909865187481046, 1.0]  d_loss:  [3.9268221855163574, 0.0]\n",
      "EPOCH:  0  BATCH:  275  g_loss: [0.0006909135263413191, 1.0]  d_loss:  [3.9273812770843506, 0.0]\n",
      "EPOCH:  0  BATCH:  276  g_loss: [0.0006909180665388703, 1.0]  d_loss:  [3.926675319671631, 0.0]\n",
      "EPOCH:  0  BATCH:  277  g_loss: [0.000690894084982574, 1.0]  d_loss:  [3.9250717163085938, 0.0]\n",
      "EPOCH:  0  BATCH:  278  g_loss: [0.0006908898940309882, 1.0]  d_loss:  [3.9239025115966797, 0.0]\n",
      "EPOCH:  0  BATCH:  279  g_loss: [0.0006908690556883812, 1.0]  d_loss:  [3.9242911338806152, 0.0]\n",
      "EPOCH:  0  BATCH:  280  g_loss: [0.0006908901268616319, 1.0]  d_loss:  [3.9253809452056885, 0.0]\n",
      "EPOCH:  0  BATCH:  281  g_loss: [0.0006908593350090086, 1.0]  d_loss:  [3.925790548324585, 0.0]\n",
      "EPOCH:  0  BATCH:  282  g_loss: [0.000690824119374156, 1.0]  d_loss:  [3.9261908531188965, 0.0]\n",
      "EPOCH:  0  BATCH:  283  g_loss: [0.0006908326176926494, 1.0]  d_loss:  [3.9221904277801514, 0.0]\n",
      "EPOCH:  0  BATCH:  284  g_loss: [0.0006908220238983631, 1.0]  d_loss:  [3.9257307052612305, 0.0]\n",
      "EPOCH:  0  BATCH:  285  g_loss: [0.0006907796487212181, 1.0]  d_loss:  [3.92756986618042, 0.0]\n",
      "EPOCH:  0  BATCH:  286  g_loss: [0.0006907882634550333, 1.0]  d_loss:  [3.9269180297851562, 0.0]\n",
      "EPOCH:  0  BATCH:  287  g_loss: [0.0006908092182129622, 1.0]  d_loss:  [3.9195804595947266, 0.0]\n",
      "EPOCH:  0  BATCH:  288  g_loss: [0.000690796528942883, 1.0]  d_loss:  [3.9255175590515137, 0.0]\n",
      "EPOCH:  0  BATCH:  289  g_loss: [0.0006907624192535877, 1.0]  d_loss:  [3.923973321914673, 0.0]\n",
      "EPOCH:  0  BATCH:  290  g_loss: [0.0006907518254593015, 1.0]  d_loss:  [3.9253790378570557, 0.0]\n",
      "EPOCH:  0  BATCH:  291  g_loss: [0.000690748100169003, 1.0]  d_loss:  [3.922390937805176, 0.0]\n",
      "EPOCH:  0  BATCH:  292  g_loss: [0.000690754852257669, 1.0]  d_loss:  [3.922659397125244, 0.0]\n",
      "EPOCH:  0  BATCH:  293  g_loss: [0.0006907213828526437, 1.0]  d_loss:  [3.923699378967285, 0.0]\n",
      "EPOCH:  0  BATCH:  294  g_loss: [0.0006907267961651087, 1.0]  d_loss:  [3.926793336868286, 0.0]\n",
      "EPOCH:  0  BATCH:  295  g_loss: [0.0006907298229634762, 1.0]  d_loss:  [3.9211812019348145, 0.0]\n",
      "EPOCH:  0  BATCH:  296  g_loss: [0.000690691638737917, 1.0]  d_loss:  [3.928144693374634, 0.0]\n",
      "EPOCH:  0  BATCH:  297  g_loss: [0.0006906943744979799, 1.0]  d_loss:  [3.925135850906372, 0.0]\n",
      "EPOCH:  0  BATCH:  298  g_loss: [0.000690676155500114, 1.0]  d_loss:  [3.925924301147461, 0.0]\n",
      "EPOCH:  0  BATCH:  299  g_loss: [0.0006906763883307576, 1.0]  d_loss:  [3.9233713150024414, 0.0]\n",
      "EPOCH:  0  BATCH:  300  g_loss: [0.0006906931521371007, 1.0]  d_loss:  [3.923856258392334, 0.0]\n",
      "EPOCH:  0  BATCH:  301  g_loss: [0.000690633722115308, 1.0]  d_loss:  [3.925743579864502, 0.0]\n",
      "EPOCH:  0  BATCH:  302  g_loss: [0.0006906482158228755, 1.0]  d_loss:  [3.923971176147461, 0.0]\n",
      "EPOCH:  0  BATCH:  303  g_loss: [0.0006906645721755922, 1.0]  d_loss:  [3.9253907203674316, 0.0]\n",
      "EPOCH:  0  BATCH:  304  g_loss: [0.0006906299968250096, 1.0]  d_loss:  [3.9241538047790527, 0.0]\n",
      "EPOCH:  0  BATCH:  305  g_loss: [0.0006905770860612392, 1.0]  d_loss:  [3.9266276359558105, 0.0]\n",
      "EPOCH:  0  BATCH:  306  g_loss: [0.0006905674235895276, 1.0]  d_loss:  [3.925729274749756, 0.0]\n",
      "EPOCH:  0  BATCH:  307  g_loss: [0.0006906185299158096, 1.0]  d_loss:  [3.922787666320801, 0.0]\n",
      "EPOCH:  0  BATCH:  308  g_loss: [0.0006906089256517589, 1.0]  d_loss:  [3.9267516136169434, 0.0]\n",
      "EPOCH:  0  BATCH:  309  g_loss: [0.0006905724876560271, 1.0]  d_loss:  [3.922999858856201, 0.0]\n",
      "EPOCH:  0  BATCH:  310  g_loss: [0.000690545653924346, 1.0]  d_loss:  [3.9279797077178955, 0.0]\n",
      "EPOCH:  0  BATCH:  311  g_loss: [0.00069056247593835, 1.0]  d_loss:  [3.9245307445526123, 0.0]\n",
      "EPOCH:  0  BATCH:  312  g_loss: [0.000690567190758884, 1.0]  d_loss:  [3.923740863800049, 0.0]\n",
      "EPOCH:  0  BATCH:  313  g_loss: [0.0006905599148012698, 1.0]  d_loss:  [3.9241089820861816, 0.0]\n",
      "EPOCH:  0  BATCH:  314  g_loss: [0.0006905840709805489, 1.0]  d_loss:  [3.9221222400665283, 0.0]\n",
      "EPOCH:  0  BATCH:  315  g_loss: [0.0006905695190653205, 1.0]  d_loss:  [3.924975633621216, 0.0]\n",
      "EPOCH:  0  BATCH:  316  g_loss: [0.0006905598565936089, 1.0]  d_loss:  [3.9270577430725098, 0.0]\n",
      "EPOCH:  0  BATCH:  317  g_loss: [0.0006906040944159031, 1.0]  d_loss:  [3.9214160442352295, 0.0]\n",
      "EPOCH:  0  BATCH:  318  g_loss: [0.0006905835471116006, 1.0]  d_loss:  [3.925201416015625, 0.0]\n",
      "EPOCH:  0  BATCH:  319  g_loss: [0.0006905708578415215, 1.0]  d_loss:  [3.9266207218170166, 0.0]\n",
      "EPOCH:  0  BATCH:  320  g_loss: [0.0006905856425873935, 1.0]  d_loss:  [3.927250385284424, 0.0]\n",
      "EPOCH:  0  BATCH:  321  g_loss: [0.0006905512418597937, 1.0]  d_loss:  [3.9244394302368164, 0.0]\n",
      "EPOCH:  0  BATCH:  322  g_loss: [0.0006905356422066689, 1.0]  d_loss:  [3.927368402481079, 0.0]\n",
      "EPOCH:  0  BATCH:  323  g_loss: [0.0006905682384967804, 1.0]  d_loss:  [3.9248785972595215, 0.0]\n",
      "EPOCH:  0  BATCH:  324  g_loss: [0.0006905525806359947, 1.0]  d_loss:  [3.924673080444336, 0.0]\n",
      "EPOCH:  0  BATCH:  325  g_loss: [0.0006905001937411726, 1.0]  d_loss:  [3.9263901710510254, 0.0]\n",
      "EPOCH:  0  BATCH:  326  g_loss: [0.000690524117089808, 1.0]  d_loss:  [3.924745559692383, 0.0]\n",
      "EPOCH:  0  BATCH:  327  g_loss: [0.0006904855836182833, 1.0]  d_loss:  [3.9282217025756836, 0.0]\n",
      "EPOCH:  0  BATCH:  328  g_loss: [0.0006904801120981574, 1.0]  d_loss:  [3.92506742477417, 0.0]\n",
      "EPOCH:  0  BATCH:  329  g_loss: [0.0006904478068463504, 1.0]  d_loss:  [3.9280829429626465, 0.0]\n",
      "EPOCH:  0  BATCH:  330  g_loss: [0.0006903454195708036, 1.0]  d_loss:  [3.924147129058838, 0.0]\n",
      "EPOCH:  0  BATCH:  331  g_loss: [0.0006902508903294802, 1.0]  d_loss:  [3.925837993621826, 0.0]\n",
      "EPOCH:  0  BATCH:  332  g_loss: [0.0006901765591464937, 1.0]  d_loss:  [3.925811290740967, 0.0]\n",
      "EPOCH:  0  BATCH:  333  g_loss: [0.0006900331936776638, 1.0]  d_loss:  [3.927485466003418, 0.0]\n",
      "EPOCH:  0  BATCH:  334  g_loss: [0.000689936219714582, 1.0]  d_loss:  [3.929668664932251, 0.0]\n",
      "EPOCH:  0  BATCH:  335  g_loss: [0.0006898300489410758, 1.0]  d_loss:  [3.9250073432922363, 0.0]\n",
      "EPOCH:  0  BATCH:  336  g_loss: [0.0006896445411257446, 1.0]  d_loss:  [3.9220423698425293, 0.0]\n",
      "EPOCH:  0  BATCH:  337  g_loss: [0.0006895391852594912, 1.0]  d_loss:  [3.9245963096618652, 0.0]\n",
      "EPOCH:  0  BATCH:  338  g_loss: [0.0006893962854519486, 1.0]  d_loss:  [3.9230971336364746, 0.0]\n",
      "EPOCH:  0  BATCH:  339  g_loss: [0.0006892698002047837, 1.0]  d_loss:  [3.9276959896087646, 0.0]\n",
      "EPOCH:  0  BATCH:  340  g_loss: [0.0006891519296914339, 1.0]  d_loss:  [3.9238271713256836, 0.0]\n",
      "EPOCH:  0  BATCH:  341  g_loss: [0.0006890611257404089, 1.0]  d_loss:  [3.9270944595336914, 0.0]\n",
      "EPOCH:  0  BATCH:  342  g_loss: [0.000688974978402257, 1.0]  d_loss:  [3.9221205711364746, 0.0]\n",
      "EPOCH:  0  BATCH:  343  g_loss: [0.0006888795760460198, 1.0]  d_loss:  [3.923570156097412, 0.0]\n",
      "EPOCH:  0  BATCH:  344  g_loss: [0.0006888024508953094, 1.0]  d_loss:  [3.924727439880371, 0.0]\n",
      "EPOCH:  0  BATCH:  345  g_loss: [0.0006888199131935835, 1.0]  d_loss:  [3.9244744777679443, 0.0]\n",
      "EPOCH:  0  BATCH:  346  g_loss: [0.0006887815543450415, 1.0]  d_loss:  [3.926807403564453, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0  BATCH:  347  g_loss: [0.0006887782365083694, 1.0]  d_loss:  [3.9287548065185547, 0.0]\n",
      "EPOCH:  0  BATCH:  348  g_loss: [0.0006887421477586031, 1.0]  d_loss:  [3.9260802268981934, 0.0]\n",
      "EPOCH:  0  BATCH:  349  g_loss: [0.0006886861519888043, 1.0]  d_loss:  [3.924323081970215, 0.0]\n",
      "EPOCH:  0  BATCH:  350  g_loss: [0.0006887029157951474, 1.0]  d_loss:  [3.9269165992736816, 0.0]\n",
      "EPOCH:  0  BATCH:  351  g_loss: [0.000688687083311379, 1.0]  d_loss:  [3.926541328430176, 0.0]\n",
      "EPOCH:  0  BATCH:  352  g_loss: [0.0006886974442750216, 1.0]  d_loss:  [3.929774761199951, 0.0]\n",
      "EPOCH:  0  BATCH:  353  g_loss: [0.0006886073970235884, 1.0]  d_loss:  [3.9256441593170166, 0.0]\n",
      "EPOCH:  0  BATCH:  354  g_loss: [0.0006885644979774952, 1.0]  d_loss:  [3.929020404815674, 0.0]\n",
      "EPOCH:  0  BATCH:  355  g_loss: [0.0006886173505336046, 1.0]  d_loss:  [3.9306230545043945, 0.0]\n",
      "EPOCH:  0  BATCH:  356  g_loss: [0.0006885975017212331, 1.0]  d_loss:  [3.9288737773895264, 0.0]\n",
      "EPOCH:  0  BATCH:  357  g_loss: [0.0006885688053444028, 1.0]  d_loss:  [3.925248146057129, 0.0]\n",
      "EPOCH:  0  BATCH:  358  g_loss: [0.0006885274196974933, 1.0]  d_loss:  [3.929145097732544, 0.0]\n",
      "EPOCH:  0  BATCH:  359  g_loss: [0.0006885137408971786, 1.0]  d_loss:  [3.927243947982788, 0.0]\n",
      "EPOCH:  0  BATCH:  360  g_loss: [0.00068851956166327, 1.0]  d_loss:  [3.9293737411499023, 0.0]\n",
      "EPOCH:  0  BATCH:  361  g_loss: [0.0006885241600684822, 1.0]  d_loss:  [3.928349494934082, 0.0]\n",
      "EPOCH:  0  BATCH:  362  g_loss: [0.0006884976755827665, 1.0]  d_loss:  [3.924752712249756, 0.0]\n",
      "EPOCH:  0  BATCH:  363  g_loss: [0.0006884579779580235, 1.0]  d_loss:  [3.9270896911621094, 0.0]\n",
      "EPOCH:  0  BATCH:  364  g_loss: [0.0006885206094011664, 1.0]  d_loss:  [3.92741060256958, 0.0]\n",
      "EPOCH:  0  BATCH:  365  g_loss: [0.0006884747417643666, 1.0]  d_loss:  [3.929535388946533, 0.0]\n",
      "EPOCH:  0  BATCH:  366  g_loss: [0.000688473111949861, 1.0]  d_loss:  [3.9273414611816406, 0.0]\n",
      "EPOCH:  0  BATCH:  367  g_loss: [0.0006884753238409758, 1.0]  d_loss:  [3.925830364227295, 0.0]\n",
      "EPOCH:  0  BATCH:  368  g_loss: [0.0006884501781314611, 1.0]  d_loss:  [3.9269726276397705, 0.0]\n",
      "EPOCH:  0  BATCH:  369  g_loss: [0.0006884301546961069, 1.0]  d_loss:  [3.927119493484497, 0.0]\n",
      "EPOCH:  0  BATCH:  370  g_loss: [0.0006884508766233921, 1.0]  d_loss:  [3.9286985397338867, 0.0]\n",
      "EPOCH:  0  BATCH:  371  g_loss: [0.0006884250906296074, 1.0]  d_loss:  [3.926891803741455, 0.0]\n",
      "EPOCH:  0  BATCH:  372  g_loss: [0.0006884017493575811, 1.0]  d_loss:  [3.9235928058624268, 0.0]\n",
      "EPOCH:  0  BATCH:  373  g_loss: [0.0006884124595671892, 1.0]  d_loss:  [3.9256434440612793, 0.0]\n",
      "EPOCH:  0  BATCH:  374  g_loss: [0.0006883928435854614, 1.0]  d_loss:  [3.9287362098693848, 0.0]\n",
      "EPOCH:  0  BATCH:  375  g_loss: [0.0006883600726723671, 1.0]  d_loss:  [3.9267220497131348, 0.0]\n",
      "EPOCH:  0  BATCH:  376  g_loss: [0.0006883591413497925, 1.0]  d_loss:  [3.925933837890625, 0.0]\n",
      "EPOCH:  0  BATCH:  377  g_loss: [0.0006883455789647996, 1.0]  d_loss:  [3.9276390075683594, 0.0]\n",
      "EPOCH:  0  BATCH:  378  g_loss: [0.0006883383030071855, 1.0]  d_loss:  [3.9247965812683105, 0.0]\n",
      "EPOCH:  0  BATCH:  379  g_loss: [0.0006883301539346576, 1.0]  d_loss:  [3.9265389442443848, 0.0]\n",
      "EPOCH:  0  BATCH:  380  g_loss: [0.000688304309733212, 1.0]  d_loss:  [3.926478385925293, 0.0]\n",
      "EPOCH:  0  BATCH:  381  g_loss: [0.0006883356836624444, 1.0]  d_loss:  [3.9257378578186035, 0.0]\n",
      "EPOCH:  0  BATCH:  382  g_loss: [0.0006882939487695694, 1.0]  d_loss:  [3.9273228645324707, 0.0]\n",
      "EPOCH:  0  BATCH:  383  g_loss: [0.0006882527959533036, 1.0]  d_loss:  [3.92875337600708, 0.0]\n",
      "EPOCH:  0  BATCH:  384  g_loss: [0.000688267289660871, 1.0]  d_loss:  [3.927029609680176, 0.0]\n",
      "EPOCH:  0  BATCH:  385  g_loss: [0.0006882260204292834, 1.0]  d_loss:  [3.9311721324920654, 0.0]\n",
      "EPOCH:  0  BATCH:  386  g_loss: [0.000688212167005986, 1.0]  d_loss:  [3.930388927459717, 0.0]\n",
      "EPOCH:  0  BATCH:  387  g_loss: [0.0006881628069095314, 1.0]  d_loss:  [3.9279019832611084, 0.0]\n",
      "EPOCH:  0  BATCH:  388  g_loss: [0.000688113272190094, 1.0]  d_loss:  [3.929867744445801, 0.0]\n",
      "EPOCH:  0  BATCH:  389  g_loss: [0.000688113272190094, 1.0]  d_loss:  [3.9250454902648926, 0.0]\n",
      "EPOCH:  0  BATCH:  390  g_loss: [0.0006880454602651298, 1.0]  d_loss:  [3.9288601875305176, 0.0]\n",
      "EPOCH:  0  BATCH:  391  g_loss: [0.0006879887077957392, 1.0]  d_loss:  [3.925121784210205, 0.0]\n",
      "EPOCH:  0  BATCH:  392  g_loss: [0.0006878803251311183, 1.0]  d_loss:  [3.9301071166992188, 0.0]\n",
      "EPOCH:  0  BATCH:  393  g_loss: [0.0006877778796479106, 1.0]  d_loss:  [3.926081657409668, 0.0]\n",
      "EPOCH:  0  BATCH:  394  g_loss: [0.0006876455154269934, 1.0]  d_loss:  [3.929593801498413, 0.0]\n",
      "EPOCH:  0  BATCH:  395  g_loss: [0.0006874402752146125, 1.0]  d_loss:  [3.9283385276794434, 0.0]\n",
      "EPOCH:  0  BATCH:  396  g_loss: [0.0006873086094856262, 1.0]  d_loss:  [3.927643299102783, 0.0]\n",
      "EPOCH:  0  BATCH:  397  g_loss: [0.0006869812496006489, 1.0]  d_loss:  [3.9313175678253174, 0.0]\n",
      "EPOCH:  0  BATCH:  398  g_loss: [0.0006866897456347942, 1.0]  d_loss:  [3.926940441131592, 0.0]\n",
      "EPOCH:  0  BATCH:  399  g_loss: [0.0006865154718980193, 1.0]  d_loss:  [3.9309520721435547, 0.0]\n",
      "EPOCH:  0  BATCH:  400  g_loss: [0.0006862891023047268, 1.0]  d_loss:  [3.9271373748779297, 0.0]\n",
      "EPOCH:  0  BATCH:  401  g_loss: [0.0006861576111987233, 1.0]  d_loss:  [3.9290192127227783, 0.0]\n",
      "EPOCH:  0  BATCH:  402  g_loss: [0.0006858983542770147, 1.0]  d_loss:  [3.9295501708984375, 0.0]\n",
      "EPOCH:  0  BATCH:  403  g_loss: [0.0006857858970761299, 1.0]  d_loss:  [3.9294230937957764, 0.0]\n",
      "EPOCH:  0  BATCH:  404  g_loss: [0.0006854871171526611, 1.0]  d_loss:  [3.9319984912872314, 0.0]\n",
      "EPOCH:  0  BATCH:  405  g_loss: [0.0006852038786746562, 1.0]  d_loss:  [3.9242656230926514, 0.0]\n",
      "EPOCH:  0  BATCH:  406  g_loss: [0.0006848525954410434, 1.0]  d_loss:  [3.9320521354675293, 0.0]\n",
      "EPOCH:  0  BATCH:  407  g_loss: [0.0006845574826002121, 1.0]  d_loss:  [3.932239294052124, 0.0]\n",
      "EPOCH:  0  BATCH:  408  g_loss: [0.0006844760500825942, 1.0]  d_loss:  [3.9277729988098145, 0.0]\n",
      "EPOCH:  0  BATCH:  409  g_loss: [0.0006844483432359993, 1.0]  d_loss:  [3.9275126457214355, 0.0]\n",
      "EPOCH:  0  BATCH:  410  g_loss: [0.0006844558520242572, 1.0]  d_loss:  [3.92970871925354, 0.0]\n",
      "EPOCH:  0  BATCH:  411  g_loss: [0.0006843680166639388, 1.0]  d_loss:  [3.931986093521118, 0.0]\n",
      "EPOCH:  0  BATCH:  412  g_loss: [0.0006842887960374355, 1.0]  d_loss:  [3.9322147369384766, 0.0]\n",
      "EPOCH:  0  BATCH:  413  g_loss: [0.0006842541042715311, 1.0]  d_loss:  [3.9277944564819336, 0.0]\n",
      "EPOCH:  0  BATCH:  414  g_loss: [0.0006841207505203784, 1.0]  d_loss:  [3.929100513458252, 0.0]\n",
      "EPOCH:  0  BATCH:  415  g_loss: [0.0006841120193712413, 1.0]  d_loss:  [3.929690361022949, 0.0]\n",
      "EPOCH:  0  BATCH:  416  g_loss: [0.0006839780835434794, 1.0]  d_loss:  [3.93295955657959, 0.0]\n",
      "EPOCH:  0  BATCH:  417  g_loss: [0.0006839694688096642, 1.0]  d_loss:  [3.9349284172058105, 0.0]\n",
      "EPOCH:  0  BATCH:  418  g_loss: [0.0006839839043095708, 1.0]  d_loss:  [3.9298830032348633, 0.0]\n",
      "EPOCH:  0  BATCH:  419  g_loss: [0.0006839637644588947, 1.0]  d_loss:  [3.930572986602783, 0.0]\n",
      "EPOCH:  0  BATCH:  420  g_loss: [0.0006839564302936196, 1.0]  d_loss:  [3.933598279953003, 0.0]\n",
      "EPOCH:  0  BATCH:  421  g_loss: [0.0006838978151790798, 1.0]  d_loss:  [3.9277989864349365, 0.0]\n",
      "EPOCH:  0  BATCH:  422  g_loss: [0.0006838569534011185, 1.0]  d_loss:  [3.9277305603027344, 0.0]\n",
      "EPOCH:  0  BATCH:  423  g_loss: [0.0006837848341092467, 1.0]  d_loss:  [3.9283289909362793, 0.0]\n",
      "EPOCH:  0  BATCH:  424  g_loss: [0.0006837510736659169, 1.0]  d_loss:  [3.9291160106658936, 0.0]\n",
      "EPOCH:  0  BATCH:  425  g_loss: [0.0006836943794041872, 1.0]  d_loss:  [3.929121494293213, 0.0]\n",
      "EPOCH:  0  BATCH:  426  g_loss: [0.0006836717948317528, 1.0]  d_loss:  [3.93588924407959, 0.0]\n",
      "EPOCH:  0  BATCH:  427  g_loss: [0.0006836242973804474, 1.0]  d_loss:  [3.9298081398010254, 0.0]\n",
      "EPOCH:  0  BATCH:  428  g_loss: [0.0006836257525719702, 1.0]  d_loss:  [3.9302611351013184, 0.0]\n",
      "EPOCH:  0  BATCH:  429  g_loss: [0.0006835996173322201, 1.0]  d_loss:  [3.9310755729675293, 0.0]\n",
      "EPOCH:  0  BATCH:  430  g_loss: [0.0006835389649495482, 1.0]  d_loss:  [3.930161476135254, 0.0]\n",
      "EPOCH:  0  BATCH:  431  g_loss: [0.0006835275562480092, 1.0]  d_loss:  [3.9293220043182373, 0.0]\n",
      "EPOCH:  0  BATCH:  432  g_loss: [0.0006835053791292012, 1.0]  d_loss:  [3.93087100982666, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0  BATCH:  433  g_loss: [0.0006835308158770204, 1.0]  d_loss:  [3.9307472705841064, 0.0]\n",
      "EPOCH:  0  BATCH:  434  g_loss: [0.0006835264503024518, 1.0]  d_loss:  [3.9322967529296875, 0.0]\n",
      "EPOCH:  0  BATCH:  435  g_loss: [0.0006835287786088884, 1.0]  d_loss:  [3.93266224861145, 0.0]\n",
      "EPOCH:  0  BATCH:  436  g_loss: [0.0006835078820586205, 1.0]  d_loss:  [3.9306137561798096, 0.0]\n",
      "EPOCH:  0  BATCH:  437  g_loss: [0.0006835563690401614, 1.0]  d_loss:  [3.9301023483276367, 0.0]\n",
      "EPOCH:  0  BATCH:  438  g_loss: [0.0006835334934294224, 1.0]  d_loss:  [3.9312050342559814, 0.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d1fb4e0f0610>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#탐지를 한경우\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1798\u001b[0m                                                     class_weight)\n\u001b[0;32m   1799\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1800\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1802\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE =128\n",
    "num_of_batches = int(X_train.shape[0] / BATCH_SIZE)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for nob in range(num_of_batches):\n",
    "        noise = np.random.normal(0,1, size=[BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "        fake_data = generator.predict(noise)\n",
    "        real_data = X_train[np.random.randint(0, X_train.shape[0],size= BATCH_SIZE)]\n",
    "        \n",
    "        X = np.concatenate((real_data, fake_data))\n",
    "        y =np.zeros(2 * BATCH_SIZE)\n",
    "        y[:BATCH_SIZE] =0.9\n",
    "        \n",
    "        detector.trainable =True\n",
    "        d_loss = detector.train_on_batch(X, y)\n",
    "        \n",
    "        #탐지를 한경우\n",
    "        detector.trainable =False\n",
    "        noise2 = np.random.normal(0,1, size=[BATCH_SIZE, NOISE_DIM])\n",
    "        y2 = np.ones(BATCH_SIZE)\n",
    "        g_loss = model.train_on_batch(noise2,y2)\n",
    "        \n",
    "        print('EPOCH: ', e, ' BATCH: ', nob, ' g_loss:',g_loss, ' d_loss: ', d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c647622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가짜 데이터 만들기\n",
    "BATCH_SIZE =128\n",
    "noise = np.random.normal(0,1, size=[BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "fake_data = generator.predict(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7439efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실제 데이터\n",
    "real_data = X_train[np.random.randint(0, X_train.shape[0],size= BATCH_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c597de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.concatenate => 두 데이터를 중첩\n",
    "X = np.concatenate((real_data, fake_data))\n",
    "y =np.zeros(2 * BATCH_SIZE)\n",
    "# 1로 하면 너무 극단적이므로 0.9로 함\n",
    "y[:BATCH_SIZE] =0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7156c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7495442628860474, 0.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.trainable =True\n",
    "# train_on_batch => 내가 지정한 만큼만 하겠다.\n",
    "detector.train_on_batch(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866aaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b034bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
